{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and Download Data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-01T17:49:27.186972Z",
          "iopub.execute_input": "2022-02-01T17:49:27.188079Z",
          "iopub.status.idle": "2022-02-01T17:50:53.075601Z",
          "shell.execute_reply.started": "2022-02-01T17:49:27.187917Z",
          "shell.execute_reply": "2022-02-01T17:50:53.07448Z"
        },
        "id": "FJb6mdjBY-he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imutils\n",
        "!pip install segmentation_models_pytorch\n",
        "!pip install captum\n",
        "!pip install albumentations\n",
        "!pip install gdown \n",
        "import gdown \n",
        "url = 'https://drive.google.com/uc?id=1HSpQOnJoqP6frkqy1GqWuaU5-wgDDFYF' \n",
        "output = 'data.zip'\n",
        "gdown.download(url, output)\n",
        "url = 'https://drive.google.com/uc?id=12b9pu931vmw0mNn2RFt-95_qTu8s_G_4' \n",
        "output = 'best_model.pth'\n",
        "gdown.download(url, output)\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2022-02-02T21:28:19.040520Z",
          "iopub.execute_input": "2022-02-02T21:28:19.041302Z",
          "iopub.status.idle": "2022-02-02T21:29:31.553022Z",
          "shell.execute_reply.started": "2022-02-02T21:28:19.041199Z",
          "shell.execute_reply": "2022-02-02T21:29:31.551842Z"
        },
        "trusted": true,
        "id": "RsQoi5LsY-hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "from collections import defaultdict, OrderedDict\n",
        "import shutil\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "from imutils import paths\n",
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import unravel_index\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "\n",
        "from PIL import *\n",
        "import albumentations as A\n",
        "import skimage\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "from captum.attr import visualization as viz\n",
        "from captum.attr import GuidedGradCam, Saliency, DeepLift, GuidedBackprop, LayerGradCam, LayerDeepLift, LayerAttribution\n",
        "from captum.metrics import sensitivity_max, infidelity\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "SUEOkB6EoUAA",
        "execution": {
          "iopub.status.busy": "2022-02-02T21:29:31.559392Z",
          "iopub.execute_input": "2022-02-02T21:29:31.560128Z",
          "iopub.status.idle": "2022-02-02T21:29:37.291547Z",
          "shell.execute_reply.started": "2022-02-02T21:29:31.560088Z",
          "shell.execute_reply": "2022-02-02T21:29:37.290720Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "1hqtDyG6Y-hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(**images):\n",
        "    n_images = len(images)\n",
        "    f, axarr = plt.subplots(1, n_images, figsize=(4 * n_images,4))\n",
        "    for idx, (name, image) in enumerate(images.items()):\n",
        "        if image.shape[0] == 3 or image.shape[0] == 2:\n",
        "            axarr[idx].imshow(np.squeeze(image.permute(1, 2, 0)))\n",
        "        else: \n",
        "            axarr[idx].imshow(np.squeeze(image))\n",
        "        axarr[idx].set_title(name.replace('_',' ').title(), fontsize=20)\n",
        "    plt.show()\n",
        "    \n",
        "class EndoscopyDataset(Dataset):\n",
        "    def __init__(self, images, masks, augmentations=None):   \n",
        "        self.input_images = images\n",
        "        self.target_masks = masks\n",
        "        self.augmentations = augmentations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_images)\n",
        "    \n",
        "    def __getitem__(self, idx): \n",
        "        img = Image.open(os.path.join(self.input_images[idx])).convert('RGB')\n",
        "        mask = Image.open(os.path.join(self.target_masks[idx])).convert('RGB')\n",
        "        img = transforms.Compose([transforms.Resize((400, 400), interpolation=transforms.InterpolationMode.NEAREST), transforms.ToTensor()])(img)\n",
        "        mask = transforms.Compose([transforms.Resize((400, 400), interpolation=transforms.InterpolationMode.NEAREST), transforms.Grayscale(), transforms.ToTensor()])(mask)\n",
        "        img = img.permute((1, 2, 0))\n",
        "        mask = mask.permute((1, 2, 0))\n",
        "        img = img.cpu().detach().numpy()\n",
        "        mask = mask.cpu().detach().numpy()\n",
        "        \n",
        "        if self.augmentations:\n",
        "            augmented = self.augmentations(image=img, mask=mask)\n",
        "            img = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "        \n",
        "        img = torch.tensor(img, dtype=torch.float)\n",
        "        img = img.permute((2, 0, 1))\n",
        "        mask = torch.tensor(mask, dtype=torch.float)\n",
        "        mask = mask.permute((2, 0, 1))\n",
        "        \n",
        "        return [img, mask]\n",
        "    \n",
        "train_batch_size = 8\n",
        "val_batch_size = 4\n",
        "test_batch_size = 4\n",
        "num_workers = 2\n",
        "\n",
        "main_dir = './KVASIR-SEG/'\n",
        "\n",
        "train_images = sorted(list(paths.list_files(main_dir + 'train/images/', contains=\"jpg\")))\n",
        "val_images = sorted(list(paths.list_files(main_dir + 'val/images/', contains=\"jpg\")))\n",
        "test_images = sorted(list(paths.list_files(main_dir + 'test/images/', contains=\"jpg\")))\n",
        "\n",
        "train_masks = sorted(list(paths.list_files(main_dir + 'train/masks/', contains=\"jpg\")))\n",
        "val_masks = sorted(list(paths.list_files(main_dir + 'val/masks/', contains=\"jpg\")))\n",
        "test_masks = sorted(list(paths.list_files(main_dir + 'test/masks/', contains=\"jpg\")))\n",
        "\n",
        "augmentations = A.Compose({\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Rotate(limit=(-90, 90)),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.Transpose(p=0.5),\n",
        "        A.GaussianBlur(p=0.5),\n",
        "        A.augmentations.geometric.transforms.Affine(scale=(0.9, 1.1), translate_percent=0.1)\n",
        "})\n",
        "\n",
        "dataset = {\n",
        "    'train': EndoscopyDataset(train_images, train_masks, augmentations), \n",
        "    'val': EndoscopyDataset(val_images, val_masks, None), \n",
        "    'test': EndoscopyDataset(test_images, test_masks, None)\n",
        "}\n",
        "\n",
        "dataloader = {\n",
        "    'train': DataLoader(dataset['train'], batch_size=train_batch_size, shuffle=True, num_workers=num_workers),\n",
        "    'val': DataLoader(dataset['val'], batch_size=val_batch_size, shuffle=True, num_workers=num_workers),\n",
        "    'test': DataLoader(dataset['test'], batch_size=test_batch_size, shuffle=False, num_workers=num_workers)\n",
        "}\n",
        "\n",
        "image, mask = dataset['train'][random.randint(0, len(dataset['train'])-1)]\n",
        "print(image.shape, image.min(), image.max())\n",
        "print(mask.shape, mask.min(), mask.max())\n",
        "visualize(\n",
        "    original_image = image,\n",
        "    grund_truth_mask = mask,\n",
        "    polyp = skimage.segmentation.mark_boundaries(image.permute(1, 2, 0).detach().cpu().numpy(), mask.detach().cpu().numpy()[0].astype(np.int64), color=(0, 0, 1), mode='outer')\n",
        ")"
      ],
      "metadata": {
        "id": "dOKGH2G-oUAL",
        "execution": {
          "iopub.status.busy": "2022-02-02T21:29:37.293649Z",
          "iopub.execute_input": "2022-02-02T21:29:37.294085Z",
          "iopub.status.idle": "2022-02-02T21:29:37.889582Z",
          "shell.execute_reply.started": "2022-02-02T21:29:37.294045Z",
          "shell.execute_reply": "2022-02-02T21:29:37.887586Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "zKRJoSYGZlTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss(smp.utils.base.Loss):\n",
        "    def __init__(self, eps=1.0, activation=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.eps = eps\n",
        "        self.activation = smp.base.modules.Activation(activation)\n",
        "        self._name = 'loss'\n",
        "\n",
        "    def forward(self, y_pr, y_gt):\n",
        "        y_pr = self.activation(y_pr)\n",
        "        return (1 - smp.utils.functional.iou(y_pr, y_gt, eps=self.eps, threshold=0.3)) + (1 - smp.utils.functional.f_score(y_pr, y_gt, beta=1, eps=self.eps, threshold=0.3)) + nn.functional.binary_cross_entropy(y_pr, y_gt)"
      ],
      "metadata": {
        "id": "BM4fPdCSZpOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = True\n",
        "epochs = 100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
        "\n",
        "loss = Loss()\n",
        "\n",
        "metrics = [\n",
        "    smp.utils.metrics.IoU(threshold=0.3),\n",
        "    smp.utils.metrics.Fscore(threshold=0.3)\n",
        "]\n",
        "\n",
        "optimizer = torch.optim.Adam([ \n",
        "    dict(params=model.parameters(), lr=0.0005),\n",
        "])\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer, T_0=1, T_mult=2, eta_min=0.00001,\n",
        ")\n",
        "\n",
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=device,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "if training:\n",
        "\n",
        "    best_model = {'loss': 0.0, 'iou_score': 0.0, 'fscore': 0.0}\n",
        "    train_logs_list, valid_logs_list = [], []\n",
        "\n",
        "    for i in range(0, epochs):\n",
        "        print('\\nEpoch: {}'.format(i))\n",
        "        train_logs = train_epoch.run(dataloader['train'])\n",
        "        valid_logs = valid_epoch.run(dataloader['val'])\n",
        "        train_logs_list.append(train_logs)\n",
        "        valid_logs_list.append(valid_logs)\n",
        "\n",
        "        if best_model['iou_score'] < valid_logs['iou_score']:\n",
        "            torch.save(model, main_dir + 'best_model.pth')\n",
        "            best_model['loss'] = valid_logs['loss']\n",
        "            best_model['iou_score'] = valid_logs['iou_score']\n",
        "            best_model['fscore'] = valid_logs['fscore']\n",
        "            print('Model saved!')"
      ],
      "metadata": {
        "id": "x3Ry4VW1Z13j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_model)"
      ],
      "metadata": {
        "id": "6oyAlIzjZ4mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_logs_df = pd.DataFrame(train_logs_list)\n",
        "valid_logs_df = pd.DataFrame(valid_logs_list)"
      ],
      "metadata": {
        "id": "qrRGyVYiZ64A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(train_logs_df.index.tolist(), train_logs_df.loss.tolist(), lw=1, label = 'Train')\n",
        "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.loss.tolist(), lw=1, label = 'Valid')\n",
        "plt.xlabel('Epochs', fontsize=20)\n",
        "plt.ylabel('Loss', fontsize=20)\n",
        "plt.title('Loss Plot', fontsize=20)\n",
        "plt.legend(loc='best', fontsize=16)\n",
        "plt.grid()\n",
        "plt.savefig('loss_plot.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ba3TIQxHZ9Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=1, label = 'Train')\n",
        "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.iou_score.tolist(), lw=1, label = 'Valid')\n",
        "plt.xlabel('Epochs', fontsize=20)\n",
        "plt.ylabel('IoU Score', fontsize=20)\n",
        "plt.title('IoU Score Plot', fontsize=20)\n",
        "plt.legend(loc='best', fontsize=16)\n",
        "plt.grid()\n",
        "plt.savefig('iou_score_plot.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LNiJ_0TrZ_wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(train_logs_df.index.tolist(), train_logs_df.fscore.tolist(), lw=1, label = 'Train')\n",
        "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.fscore.tolist(), lw=1, label = 'Valid')\n",
        "plt.xlabel('Epochs', fontsize=20)\n",
        "plt.ylabel('F1 Score', fontsize=20)\n",
        "plt.title('F1 Score Plot', fontsize=20)\n",
        "plt.legend(loc='best', fontsize=16)\n",
        "plt.grid()\n",
        "plt.savefig('fscore_plot.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2ai8AE4PaCAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(train_logs_df.index.tolist(), train_logs_df.accuracy.tolist(), lw=1, label = 'Train')\n",
        "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.accuracy.tolist(), lw=1, label = 'Valid')\n",
        "plt.xlabel('Epochs', fontsize=20)\n",
        "plt.ylabel('Accuracy Score', fontsize=20)\n",
        "plt.title('Accuracy Score Plot', fontsize=20)\n",
        "plt.legend(loc='best', fontsize=16)\n",
        "plt.grid()\n",
        "plt.savefig('accuracy_plot.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uIbMxYuCaEZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Best Model"
      ],
      "metadata": {
        "id": "DRjwOoW9Y-hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# best_model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
        "best_model = torch.load('./best_model.pth')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-02T21:29:42.199464Z",
          "iopub.execute_input": "2022-02-02T21:29:42.199722Z",
          "iopub.status.idle": "2022-02-02T21:29:54.851693Z",
          "shell.execute_reply.started": "2022-02-02T21:29:42.199693Z",
          "shell.execute_reply": "2022-02-02T21:29:54.850884Z"
        },
        "trusted": true,
        "id": "UlELY0PqY-ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Model"
      ],
      "metadata": {
        "id": "M3JyfQNTY-hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "best_model.eval()\n",
        "\n",
        "IOUs = []\n",
        "F1s = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, labels) in enumerate(dataloader['test']):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        pred_mask = best_model(inputs)\n",
        "\n",
        "        for i in range(len(inputs)):\n",
        "            test_image = inputs[i]\n",
        "            test_mask = labels[i]\n",
        "            predMask = pred_mask[i]\n",
        "            \n",
        "            iou = smp.utils.functional.iou(predMask, test_mask, threshold=0.3)\n",
        "            IOUs.append(iou.cpu().detach())\n",
        "\n",
        "            f1 = smp.utils.functional.f_score(predMask, test_mask, threshold=0.3)\n",
        "            F1s.append(f1.cpu().detach())\n",
        "            \n",
        "            predMask = torch.where(predMask >= 0.3, 1, 0)"
      ],
      "metadata": {
        "id": "pj0hVHuioUAW",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2022-02-02T21:29:54.853549Z",
          "iopub.execute_input": "2022-02-02T21:29:54.853802Z",
          "iopub.status.idle": "2022-02-02T21:30:01.744419Z",
          "shell.execute_reply.started": "2022-02-02T21:29:54.853765Z",
          "shell.execute_reply": "2022-02-02T21:30:01.743515Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test IOU: ' + str(np.mean(IOUs)))\n",
        "print('Test F1: ' + str(np.mean(F1s)))"
      ],
      "metadata": {
        "id": "8IfIxNS8oUAX",
        "execution": {
          "iopub.status.busy": "2022-02-02T21:30:01.746115Z",
          "iopub.execute_input": "2022-02-02T21:30:01.746389Z",
          "iopub.status.idle": "2022-02-02T21:30:01.805429Z",
          "shell.execute_reply.started": "2022-02-02T21:30:01.746352Z",
          "shell.execute_reply": "2022-02-02T21:30:01.804662Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpret"
      ],
      "metadata": {
        "id": "mB9I6aDxY-hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_border(mask):\n",
        "    border = skimage.segmentation.find_boundaries(mask.detach().cpu().numpy(), mode='outer').astype(np.uint8)\n",
        "    indices = np.where(border == 1)\n",
        "    indices = np.concatenate((indices[0][...,np.newaxis],indices[1][...,np.newaxis],indices[2][...,np.newaxis]),axis=1)\n",
        "    return list(map(tuple, indices))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-02T21:41:33.018496Z",
          "iopub.execute_input": "2022-02-02T21:41:33.018777Z",
          "iopub.status.idle": "2022-02-02T21:41:33.075152Z",
          "shell.execute_reply.started": "2022-02-02T21:41:33.018746Z",
          "shell.execute_reply": "2022-02-02T21:41:33.074285Z"
        },
        "trusted": true,
        "id": "D0OLEaHgY-hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attr(method, image, targets):\n",
        "    maps = list()\n",
        "    for target in targets:\n",
        "        if isinstance(method, DeepLift):\n",
        "            attr = method.attribute(image.to(device), target=target, return_convergence_delta=False)\n",
        "        else:\n",
        "            attr = method.attribute(image.to(device), target=target)\n",
        "        if attr.shape[2] < image.shape[2]:\n",
        "            upsampled_attr = LayerAttribution.interpolate(attr, (image.shape[2], image.shape[3]))\n",
        "        else:\n",
        "            upsampled_attr = attr\n",
        "        maps.append(np.mean(upsampled_attr.detach().cpu().numpy()[0], 0, keepdims=True)[0])\n",
        "    return np.array(maps)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-02T21:41:36.897575Z",
          "iopub.execute_input": "2022-02-02T21:41:36.897828Z",
          "iopub.status.idle": "2022-02-02T21:41:36.949636Z",
          "shell.execute_reply.started": "2022-02-02T21:41:36.897800Z",
          "shell.execute_reply": "2022-02-02T21:41:36.948760Z"
        },
        "trusted": true,
        "id": "2XjVvAR0Y-hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agg_attr(out, size):\n",
        "    i = np.zeros(out.shape)\n",
        "    for k in range(out.shape[0]):\n",
        "        a = np.max(np.abs(out[k]))\n",
        "        a = a if a != 0 else 1\n",
        "        i[k] = out[k] / a\n",
        "    return np.sum(i, 0).reshape(size, size, 1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-02T21:41:38.621383Z",
          "iopub.execute_input": "2022-02-02T21:41:38.622206Z",
          "iopub.status.idle": "2022-02-02T21:41:38.671898Z",
          "shell.execute_reply.started": "2022-02-02T21:41:38.622155Z",
          "shell.execute_reply": "2022-02-02T21:41:38.671139Z"
        },
        "trusted": true,
        "id": "URQcyTmfY-hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interpret_model(model, image, image_name):\n",
        "    figure_size = (5, 5)\n",
        "    img_cpu = image.cpu().permute(1, 2, 0).detach().numpy()\n",
        "    img_batch = image.unsqueeze(0)\n",
        "    pred_mask = model(img_batch.to(device))[0]\n",
        "    binary_mask = torch.where(pred_mask >= 0.3, 1, 0)\n",
        "    \n",
        "    sm = Saliency(model)\n",
        "    sm_out = get_attr(sm, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(sm_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Saliency.png', format='png', dpi=1200)\n",
        "\n",
        "    gbp = GuidedBackprop(model)\n",
        "    gbp_out = get_attr(gbp, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(gbp_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Guided Backpropagation.png', format='png', dpi=1200)\n",
        "    \n",
        "    ggc = GuidedGradCam(model, model.conv)\n",
        "    ggc_out = get_attr(ggc, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ggc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Guided Grad-CAM.png', format='png', dpi=1200)\n",
        "    \n",
        "    dl = DeepLift(model)\n",
        "    dl_out = get_attr(dl, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(dl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_DeepLift.png', format='png', dpi=1200)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-02T21:42:27.354461Z",
          "iopub.execute_input": "2022-02-02T21:42:27.354708Z",
          "iopub.status.idle": "2022-02-02T21:42:27.411056Z",
          "shell.execute_reply.started": "2022-02-02T21:42:27.354681Z",
          "shell.execute_reply": "2022-02-02T21:42:27.410357Z"
        },
        "trusted": true,
        "id": "TD0D6QHQY-hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interpret_layers_with_gradcam(model, image, image_name):\n",
        "    figure_size = (5, 5)\n",
        "    img_cpu = image.cpu().permute(1, 2, 0).detach().numpy()\n",
        "    img_batch = image.unsqueeze(0)\n",
        "    pred_mask = model(img_batch.to(device))[0]\n",
        "    binary_mask = torch.where(pred_mask >= 0.3, 1, 0)\n",
        "\n",
        "    lgc = LayerGradCam(model, model.encoder1)\n",
        "    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"],\n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Encoder1\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer Grad-CAM_Encoder1.png', format='png', dpi=1200)\n",
        "    \n",
        "    lgc = LayerGradCam(model, model.encoder2)\n",
        "    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Encoder2\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer Grad-CAM_Encoder2.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    lgc = LayerGradCam(model, model.encoder3)\n",
        "    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Encoder3\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer Grad-CAM_Encoder3.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    lgc = LayerGradCam(model, model.encoder4)\n",
        "    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Encoder4\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer Grad-CAM_Encoder4.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    lgc = LayerGradCam(model, model.bottleneck)\n",
        "    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Bottleneck\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer Grad-CAM_Bottleneck.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    lgc = LayerGradCam(model, model.decoder4)\n",
        "    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Decoder4\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer Grad-CAM_Decoder4.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    lgc = LayerGradCam(model, model.decoder3)\n",
        "    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Decoder3\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer Grad-CAM_Decoder3.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    lgc = LayerGradCam(model, model.decoder2)\n",
        "    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Decoder2\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer Grad-CAM_Decoder2.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    lgc = LayerGradCam(model, model.decoder1)\n",
        "    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Decoder1\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer Grad-CAM_Decoder1.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    lgc = LayerGradCam(model, model.conv)\n",
        "    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Last Layer\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer Grad-CAM_LastLayer.png', format='png', dpi=1200)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-02T21:41:44.882301Z",
          "iopub.execute_input": "2022-02-02T21:41:44.882565Z",
          "iopub.status.idle": "2022-02-02T21:41:44.950905Z",
          "shell.execute_reply.started": "2022-02-02T21:41:44.882533Z",
          "shell.execute_reply": "2022-02-02T21:41:44.950157Z"
        },
        "trusted": true,
        "id": "aBJSSuMsY-hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interpret_layers_with_deeplift(model, image, image_name):\n",
        "    figure_size = (5, 5)\n",
        "    img_cpu = image.cpu().permute(1, 2, 0).detach().numpy()\n",
        "    img_batch = image.unsqueeze(0)\n",
        "    pred_mask = model(img_batch.to(device))[0]\n",
        "    binary_mask = torch.where(pred_mask >= 0.3, 1, 0)\n",
        "                                      \n",
        "    ldl = LayerDeepLift(model, model.encoder1)\n",
        "    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Encoder1\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer DeepLift_Encoder1.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    ldl = LayerDeepLift(model, model.encoder2)\n",
        "    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Encoder2\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer DeepLift_Encoder2.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    ldl = LayerDeepLift(model, model.encoder3)\n",
        "    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Encoder3\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer DeepLift_Encoder3.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    ldl = LayerDeepLift(model, model.encoder4)\n",
        "    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Encoder4\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer DeepLift_Encoder4.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    ldl = LayerDeepLift(model, model.bottleneck)\n",
        "    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Bottleneck\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer DeepLift_Bottleneck.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    ldl = LayerDeepLift(model, model.decoder4)\n",
        "    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Decoder4\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer DeepLift_Decoder4.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    ldl = LayerDeepLift(model, model.decoder3)\n",
        "    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Decoder3\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer DeepLift_Decoder3.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    ldl = LayerDeepLift(model, model.decoder2)\n",
        "    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu,signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Decoder2\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer DeepLift_Decoder2.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    ldl = LayerDeepLift(model, model.decoder1)\n",
        "    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Decoder1\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer DeepLift_Decoder1.png', format='png', dpi=1200)\n",
        "                                      \n",
        "    ldl = LayerDeepLift(model, model.conv)\n",
        "    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n",
        "    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n",
        "                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Last Layer\"], fig_size=figure_size)\n",
        "    figure.savefig(image_name + '_Layer DeepLift_LastLayer.png', format='png', dpi=1200)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-02T21:41:48.523576Z",
          "iopub.execute_input": "2022-02-02T21:41:48.523826Z",
          "iopub.status.idle": "2022-02-02T21:41:48.631643Z",
          "shell.execute_reply.started": "2022-02-02T21:41:48.523798Z",
          "shell.execute_reply": "2022-02-02T21:41:48.630984Z"
        },
        "trusted": true,
        "id": "MatDDqrgY-h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Interpretions"
      ],
      "metadata": {
        "id": "IfJtko27Y-h5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perturb_fn(inputs):\n",
        "    noise = torch.tensor(np.random.normal(0, 0.001, inputs.shape)).float().to(device)\n",
        "    return noise, inputs - noise"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-02T18:34:27.204437Z",
          "iopub.execute_input": "2022-02-02T18:34:27.204723Z",
          "iopub.status.idle": "2022-02-02T18:34:27.264593Z",
          "shell.execute_reply.started": "2022-02-02T18:34:27.204691Z",
          "shell.execute_reply": "2022-02-02T18:34:27.263423Z"
        },
        "trusted": true,
        "id": "5HnSo31vY-h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infidelity_score_model_interpretations(model, image):\n",
        "    \n",
        "    methods = [Saliency(model), GuidedBackprop(model), GuidedGradCam(model, model.conv), DeepLift(model)]\n",
        "    infidelity_scores = [0, 0, 0, 0]\n",
        "    \n",
        "    img_cpu = image.cpu().permute(1, 2, 0).detach().numpy()\n",
        "    img_batch = image.unsqueeze(0)\n",
        "    pred_mask = model(img_batch.to(device))[0]\n",
        "    binary_mask = torch.where(pred_mask >= 0.3, 1, 0)\n",
        "    border = get_border(binary_mask)\n",
        "    \n",
        "    for i in range(len(methods)):\n",
        "        infid = 0\n",
        "        for j in range(len(border)):\n",
        "            if isinstance(methods[i], DeepLift):\n",
        "                attribution = methods[i].attribute(img_batch.to(device), target=border[j], return_convergence_delta=False)\n",
        "            else:\n",
        "                attribution = methods[i].attribute(img_batch.to(device), target=border[j])\n",
        "            infid += infidelity(model, perturb_fn, img_batch.to(device), attribution, n_perturb_samples=1)\n",
        "        infid /= len(border)\n",
        "        infidelity_scores[i] += infid\n",
        "    \n",
        "    return infidelity_scores"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-02T16:40:17.761287Z",
          "iopub.execute_input": "2022-02-02T16:40:17.761896Z",
          "iopub.status.idle": "2022-02-02T16:40:17.826401Z",
          "shell.execute_reply.started": "2022-02-02T16:40:17.76186Z",
          "shell.execute_reply": "2022-02-02T16:40:17.825412Z"
        },
        "trusted": true,
        "id": "61cJpxRoY-iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infidelity_score_layer_interpretations(model, test_dataset):\n",
        "    \n",
        "    methods = [[LayerGradCam(model, model.encoder1), LayerGradCam(model, model.encoder2), LayerGradCam(model, model.encoder3), \n",
        "                LayerGradCam(model, model.encoder4), LayerGradCam(model, model.bottleneck), LayerGradCam(model, model.decoder4), \n",
        "                LayerGradCam(model, model.decoder3), LayerGradCam(model, model.decoder2), LayerGradCam(model, model.decoder1), \n",
        "                LayerGradCam(model, model.conv)], \n",
        "               [LayerDeepLift(model, model.encoder1), LayerDeepLift(model, model.encoder2), LayerDeepLift(model, model.encoder3), \n",
        "                LayerDeepLift(model, model.encoder4), LayerDeepLift(model, model.bottleneck), LayerDeepLift(model, model.decoder4), \n",
        "                LayerDeepLift(model, model.decoder3), LayerDeepLift(model, model.decoder2), LayerDeepLift(model, model.decoder1), \n",
        "                LayerDeepLift(model, model.conv)]]\n",
        "    infidelity_scores = [0, 0]\n",
        "    \n",
        "    img_cpu = image.cpu().permute(1, 2, 0).detach().numpy()\n",
        "    img_batch = image.unsqueeze(0)\n",
        "    pred_mask = model(img_batch.to(device))[0]\n",
        "    binary_mask = torch.where(pred_mask >= 0.3, 1, 0)\n",
        "    border = get_border(binary_mask)\n",
        "        \n",
        "    for i in range(len(methods)):\n",
        "        infid = 0\n",
        "        for k in range(len(methods[i])):\n",
        "            layer_infid = 0\n",
        "            for j in range(len(border)):\n",
        "                attribution = methods[i][k].attribute(img_batch.to(device), target=border[j])\n",
        "                if attribution.shape[2] < image.shape[1]:\n",
        "                    attribution = LayerAttribution.interpolate(attribution, (image.shape[1], image.shape[2]))\n",
        "                if isinstance(methods[i][k], LayerDeepLift):\n",
        "                    attribution = torch.mean(attribution, 1, keepdims=True)\n",
        "                layer_infid += infidelity(model, perturb_fn, img_batch.to(device), attribution.repeat(1, 3, 1, 1), n_perturb_samples=1)\n",
        "            layer_infid /= len(border)\n",
        "            infid += layer_infid\n",
        "        infid /= len(methods[i])\n",
        "        infidelity_scores[i] += infid\n",
        "    \n",
        "    return infidelity_scores"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-02T18:34:34.986144Z",
          "iopub.execute_input": "2022-02-02T18:34:34.987093Z",
          "iopub.status.idle": "2022-02-02T18:34:35.059782Z",
          "shell.execute_reply.started": "2022-02-02T18:34:34.987052Z",
          "shell.execute_reply": "2022-02-02T18:34:35.058677Z"
        },
        "trusted": true,
        "id": "wWxOROhvY-iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_infid = []\n",
        "for idx in range(len(dataset['test'])):\n",
        "    img, _ = dataset['test'][idx]\n",
        "    model_infid.append(infidelity_score_model_interpretations(best_model, img))\n",
        "print(np.mean(model_infid))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-02T16:41:15.222382Z",
          "iopub.execute_input": "2022-02-02T16:41:15.223176Z",
          "iopub.status.idle": "2022-02-02T17:07:46.126458Z",
          "shell.execute_reply.started": "2022-02-02T16:41:15.223126Z",
          "shell.execute_reply": "2022-02-02T17:07:46.125515Z"
        },
        "trusted": true,
        "id": "_Nxe1yrLY-jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_infid = []\n",
        "for idx in range(len(dataset['test'])):\n",
        "    img, _ = dataset['test'][idx]\n",
        "    layer_infid.append(infidelity_score_layer_interpretations(best_model, img))\n",
        "print(np.mean(layer_infid))"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2022-02-02T18:34:39.831276Z",
          "iopub.execute_input": "2022-02-02T18:34:39.83195Z",
          "iopub.status.idle": "2022-02-02T20:30:00.83779Z",
          "shell.execute_reply.started": "2022-02-02T18:34:39.83189Z",
          "shell.execute_reply": "2022-02-02T20:30:00.836726Z"
        },
        "trusted": true,
        "id": "y8QDkttgY-jC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}