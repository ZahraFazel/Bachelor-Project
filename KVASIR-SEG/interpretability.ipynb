{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries and Download Data","metadata":{"execution":{"iopub.status.busy":"2022-02-01T17:49:27.186972Z","iopub.execute_input":"2022-02-01T17:49:27.188079Z","iopub.status.idle":"2022-02-01T17:50:53.075601Z","shell.execute_reply.started":"2022-02-01T17:49:27.187917Z","shell.execute_reply":"2022-02-01T17:50:53.07448Z"}}},{"cell_type":"code","source":"!pip install imutils\n!pip install segmentation_models_pytorch\n!pip install captum\n!pip install albumentations\n!pip install gdown \nimport gdown \nurl = 'https://drive.google.com/uc?id=1m6YzFu1ejIw9mZYLq17o0qmdv7ZOZbbn' \noutput = 'data.zip'\ngdown.download(url, output)\nurl = 'https://drive.google.com/uc?id=1Y93BKeKrvlwrieQc9aPkrvzwaUjWwE2Z' \noutput = 'best_model.pth'\ngdown.download(url, output)\n!unzip data.zip","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-02T21:28:19.040520Z","iopub.execute_input":"2022-02-02T21:28:19.041302Z","iopub.status.idle":"2022-02-02T21:29:31.553022Z","shell.execute_reply.started":"2022-02-02T21:28:19.041199Z","shell.execute_reply":"2022-02-02T21:29:31.551842Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n\nimport os\nfrom collections import defaultdict, OrderedDict\nimport shutil\nimport time\nimport copy\nimport math\nimport random\nfrom imutils import paths\nimport warnings\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy import unravel_index\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nfrom torchvision import transforms\nfrom torchvision import datasets\n\nfrom PIL import *\nimport albumentations as A\nimport skimage\n\nimport segmentation_models_pytorch as smp\n\nfrom captum.attr import visualization as viz\nfrom captum.attr import GuidedGradCam, Saliency, DeepLift, GuidedBackprop, LayerGradCam, LayerDeepLift, LayerAttribution\nfrom captum.metrics import sensitivity_max, infidelity\n\nwarnings.filterwarnings('ignore')\nprint(torch.cuda.is_available())","metadata":{"id":"SUEOkB6EoUAA","outputId":"d0151148-10bd-4b6b-98f3-27d3aec82203","execution":{"iopub.status.busy":"2022-02-02T21:29:31.559392Z","iopub.execute_input":"2022-02-02T21:29:31.560128Z","iopub.status.idle":"2022-02-02T21:29:37.291547Z","shell.execute_reply.started":"2022-02-02T21:29:31.560088Z","shell.execute_reply":"2022-02-02T21:29:37.290720Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"def visualize(**images):\n    n_images = len(images)\n    f, axarr = plt.subplots(1, n_images, figsize=(4 * n_images,4))\n    for idx, (name, image) in enumerate(images.items()):\n        if image.shape[0] == 3 or image.shape[0] == 2:\n            axarr[idx].imshow(np.squeeze(image.permute(1, 2, 0)))\n        else: \n            axarr[idx].imshow(np.squeeze(image))\n        axarr[idx].set_title(name.replace('_',' ').title(), fontsize=20)\n    plt.show()\n    \nclass EndoscopyDataset(Dataset):\n    def __init__(self, images, masks, augmentations=None):   \n        self.input_images = images\n        self.target_masks = masks\n        self.augmentations = augmentations\n\n    def __len__(self):\n        return len(self.input_images)\n    \n    def __getitem__(self, idx): \n        img = Image.open(os.path.join(self.input_images[idx])).convert('RGB')\n        mask = Image.open(os.path.join(self.target_masks[idx])).convert('RGB')\n        img = transforms.Compose([transforms.Resize((400, 400), interpolation=transforms.InterpolationMode.NEAREST), transforms.ToTensor()])(img)\n        mask = transforms.Compose([transforms.Resize((400, 400), interpolation=transforms.InterpolationMode.NEAREST), transforms.Grayscale(), transforms.ToTensor()])(mask)\n        img = img.permute((1, 2, 0))\n        mask = mask.permute((1, 2, 0))\n        img = img.cpu().detach().numpy()\n        mask = mask.cpu().detach().numpy()\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=img, mask=mask)\n            img = augmented['image']\n            mask = augmented['mask']\n        \n        img = torch.tensor(img, dtype=torch.float)\n        img = img.permute((2, 0, 1))\n        mask = torch.tensor(mask, dtype=torch.float)\n        mask = mask.permute((2, 0, 1))\n        \n        return [img, mask]\n    \ntrain_batch_size = 8\nval_batch_size = 4\ntest_batch_size = 4\nnum_workers = 2\n\nmain_dir = './'\n\ntrain_images = sorted(list(paths.list_files(main_dir + 'train/images/', contains=\"jpg\")))\nval_images = sorted(list(paths.list_files(main_dir + 'val/images/', contains=\"jpg\")))\ntest_images = sorted(list(paths.list_files(main_dir + 'test/images/', contains=\"jpg\")))\n\ntrain_masks = sorted(list(paths.list_files(main_dir + 'train/masks/', contains=\"jpg\")))\nval_masks = sorted(list(paths.list_files(main_dir + 'val/masks/', contains=\"jpg\")))\ntest_masks = sorted(list(paths.list_files(main_dir + 'test/masks/', contains=\"jpg\")))\n\naugmentations = A.Compose({\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(limit=(-90, 90)),\n        A.VerticalFlip(p=0.5),\n        A.Transpose(p=0.5),\n        A.GaussianBlur(p=0.5),\n        A.augmentations.geometric.transforms.Affine(scale=(0.9, 1.1), translate_percent=0.1)\n})\n\ndataset = {\n    'train': EndoscopyDataset(train_images, train_masks, augmentations), \n    'val': EndoscopyDataset(val_images, val_masks, None), \n    'test': EndoscopyDataset(test_images, test_masks, None)\n}\n\ndataloader = {\n    'train': DataLoader(dataset['train'], batch_size=train_batch_size, shuffle=True, num_workers=num_workers),\n    'val': DataLoader(dataset['val'], batch_size=val_batch_size, shuffle=True, num_workers=num_workers),\n    'test': DataLoader(dataset['test'], batch_size=test_batch_size, shuffle=False, num_workers=num_workers)\n}\n\nimage, mask = dataset['train'][random.randint(0, len(dataset['train'])-1)]\nprint(image.shape, image.min(), image.max())\nprint(mask.shape, mask.min(), mask.max())\nvisualize(\n    original_image = image,\n    grund_truth_mask = mask,\n    polyp = skimage.segmentation.mark_boundaries(image.permute(1, 2, 0).detach().cpu().numpy(), mask.detach().cpu().numpy()[0].astype(np.int64), color=(0, 0, 1), mode='outer')\n)","metadata":{"id":"dOKGH2G-oUAL","execution":{"iopub.status.busy":"2022-02-02T21:29:37.293649Z","iopub.execute_input":"2022-02-02T21:29:37.294085Z","iopub.status.idle":"2022-02-02T21:29:37.889582Z","shell.execute_reply.started":"2022-02-02T21:29:37.294045Z","shell.execute_reply":"2022-02-02T21:29:37.887586Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbest_model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\nbest_model = torch.load('./best_model.pth')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T21:29:42.199464Z","iopub.execute_input":"2022-02-02T21:29:42.199722Z","iopub.status.idle":"2022-02-02T21:29:54.851693Z","shell.execute_reply.started":"2022-02-02T21:29:42.199693Z","shell.execute_reply":"2022-02-02T21:29:54.850884Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Test Model","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nbest_model.eval()\n\nIOUs = []\nF1s = []\n\nwith torch.no_grad():\n    for i, (inputs, labels) in enumerate(dataloader['test']):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        pred_mask = best_model(inputs)\n\n        for i in range(len(inputs)):\n            test_image = inputs[i]\n            test_mask = labels[i]\n            predMask = pred_mask[i]\n            \n            iou = smp.utils.functional.iou(predMask, test_mask, threshold=0.3)\n            IOUs.append(iou.cpu().detach())\n\n            f1 = smp.utils.functional.f_score(predMask, test_mask, threshold=0.3)\n            F1s.append(f1.cpu().detach())\n            \n            predMask = torch.where(predMask >= 0.3, 1, 0)","metadata":{"id":"pj0hVHuioUAW","outputId":"aae25f2b-f76f-40e0-9825-4f61117fa483","scrolled":true,"execution":{"iopub.status.busy":"2022-02-02T21:29:54.853549Z","iopub.execute_input":"2022-02-02T21:29:54.853802Z","iopub.status.idle":"2022-02-02T21:30:01.744419Z","shell.execute_reply.started":"2022-02-02T21:29:54.853765Z","shell.execute_reply":"2022-02-02T21:30:01.743515Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print('Test IOU: ' + str(np.mean(IOUs)))\nprint('Test F1: ' + str(np.mean(F1s)))","metadata":{"id":"8IfIxNS8oUAX","outputId":"18e27cec-5791-4717-d384-c8f852533409","execution":{"iopub.status.busy":"2022-02-02T21:30:01.746115Z","iopub.execute_input":"2022-02-02T21:30:01.746389Z","iopub.status.idle":"2022-02-02T21:30:01.805429Z","shell.execute_reply.started":"2022-02-02T21:30:01.746352Z","shell.execute_reply":"2022-02-02T21:30:01.804662Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Interpret","metadata":{}},{"cell_type":"code","source":"def get_border(mask):\n    border = skimage.segmentation.find_boundaries(mask.detach().cpu().numpy(), mode='outer').astype(np.uint8)\n    indices = np.where(border == 1)\n    indices = np.concatenate((indices[0][...,np.newaxis],indices[1][...,np.newaxis],indices[2][...,np.newaxis]),axis=1)\n    return list(map(tuple, indices))","metadata":{"execution":{"iopub.status.busy":"2022-02-02T21:41:33.018496Z","iopub.execute_input":"2022-02-02T21:41:33.018777Z","iopub.status.idle":"2022-02-02T21:41:33.075152Z","shell.execute_reply.started":"2022-02-02T21:41:33.018746Z","shell.execute_reply":"2022-02-02T21:41:33.074285Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def get_attr(method, image, targets):\n    maps = list()\n    for target in targets:\n        if isinstance(method, DeepLift):\n            attr = method.attribute(image.to(device), target=target, return_convergence_delta=False)\n        else:\n            attr = method.attribute(image.to(device), target=target)\n        if attr.shape[2] < image.shape[2]:\n            upsampled_attr = LayerAttribution.interpolate(attr, (image.shape[2], image.shape[3]))\n        else:\n            upsampled_attr = attr\n        maps.append(np.mean(upsampled_attr.detach().cpu().numpy()[0], 0, keepdims=True)[0])\n    return np.array(maps)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T21:41:36.897575Z","iopub.execute_input":"2022-02-02T21:41:36.897828Z","iopub.status.idle":"2022-02-02T21:41:36.949636Z","shell.execute_reply.started":"2022-02-02T21:41:36.897800Z","shell.execute_reply":"2022-02-02T21:41:36.948760Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def agg_attr(out, size):\n    i = np.zeros(out.shape)\n    for k in range(out.shape[0]):\n        a = np.max(np.abs(out[k]))\n        a = a if a != 0 else 1\n        i[k] = out[k] / a\n    return np.sum(i, 0).reshape(size, size, 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T21:41:38.621383Z","iopub.execute_input":"2022-02-02T21:41:38.622206Z","iopub.status.idle":"2022-02-02T21:41:38.671898Z","shell.execute_reply.started":"2022-02-02T21:41:38.622155Z","shell.execute_reply":"2022-02-02T21:41:38.671139Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def interpret_model(model, image, image_name):\n    figure_size = (5, 5)\n    img_cpu = image.cpu().permute(1, 2, 0).detach().numpy()\n    img_batch = image.unsqueeze(0)\n    pred_mask = model(img_batch.to(device))[0]\n    binary_mask = torch.where(pred_mask >= 0.3, 1, 0)\n    \n    sm = Saliency(model)\n    sm_out = get_attr(sm, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(sm_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, fig_size=figure_size)\n    figure.savefig(image_name + '_Saliency.png', format='png', dpi=1200)\n\n    gbp = GuidedBackprop(model)\n    gbp_out = get_attr(gbp, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(gbp_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, fig_size=figure_size)\n    figure.savefig(image_name + '_Guided Backpropagation.png', format='png', dpi=1200)\n    \n    ggc = GuidedGradCam(model, model.conv)\n    ggc_out = get_attr(ggc, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ggc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, fig_size=figure_size)\n    figure.savefig(image_name + '_Guided Grad-CAM.png', format='png', dpi=1200)\n    \n    dl = DeepLift(model)\n    dl_out = get_attr(dl, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(dl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, fig_size=figure_size)\n    figure.savefig(image_name + '_DeepLift.png', format='png', dpi=1200)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T21:42:27.354461Z","iopub.execute_input":"2022-02-02T21:42:27.354708Z","iopub.status.idle":"2022-02-02T21:42:27.411056Z","shell.execute_reply.started":"2022-02-02T21:42:27.354681Z","shell.execute_reply":"2022-02-02T21:42:27.410357Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def interpret_layers_with_gradcam(model, image, image_name):\n    figure_size = (5, 5)\n    img_cpu = image.cpu().permute(1, 2, 0).detach().numpy()\n    img_batch = image.unsqueeze(0)\n    pred_mask = model(img_batch.to(device))[0]\n    binary_mask = torch.where(pred_mask >= 0.3, 1, 0)\n\n    lgc = LayerGradCam(model, model.encoder1)\n    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"],\n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Encoder1\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer Grad-CAM_Encoder1.png', format='png', dpi=1200)\n    \n    lgc = LayerGradCam(model, model.encoder2)\n    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Encoder2\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer Grad-CAM_Encoder2.png', format='png', dpi=1200)\n                                      \n    lgc = LayerGradCam(model, model.encoder3)\n    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Encoder3\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer Grad-CAM_Encoder3.png', format='png', dpi=1200)\n                                      \n    lgc = LayerGradCam(model, model.encoder4)\n    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Encoder4\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer Grad-CAM_Encoder4.png', format='png', dpi=1200)\n                                      \n    lgc = LayerGradCam(model, model.bottleneck)\n    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Bottleneck\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer Grad-CAM_Bottleneck.png', format='png', dpi=1200)\n                                      \n    lgc = LayerGradCam(model, model.decoder4)\n    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Decoder4\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer Grad-CAM_Decoder4.png', format='png', dpi=1200)\n                                      \n    lgc = LayerGradCam(model, model.decoder3)\n    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Decoder3\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer Grad-CAM_Decoder3.png', format='png', dpi=1200)\n                                      \n    lgc = LayerGradCam(model, model.decoder2)\n    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Decoder2\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer Grad-CAM_Decoder2.png', format='png', dpi=1200)\n                                      \n    lgc = LayerGradCam(model, model.decoder1)\n    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Decoder1\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer Grad-CAM_Decoder1.png', format='png', dpi=1200)\n                                      \n    lgc = LayerGradCam(model, model.conv)\n    lgc_out = get_attr(lgc, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(lgc_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"Grad-CAM - Last Layer\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer Grad-CAM_LastLayer.png', format='png', dpi=1200)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T21:41:44.882301Z","iopub.execute_input":"2022-02-02T21:41:44.882565Z","iopub.status.idle":"2022-02-02T21:41:44.950905Z","shell.execute_reply.started":"2022-02-02T21:41:44.882533Z","shell.execute_reply":"2022-02-02T21:41:44.950157Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def interpret_layers_with_deeplift(model, image, image_name):\n    figure_size = (5, 5)\n    img_cpu = image.cpu().permute(1, 2, 0).detach().numpy()\n    img_batch = image.unsqueeze(0)\n    pred_mask = model(img_batch.to(device))[0]\n    binary_mask = torch.where(pred_mask >= 0.3, 1, 0)\n                                      \n    ldl = LayerDeepLift(model, model.encoder1)\n    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Encoder1\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer DeepLift_Encoder1.png', format='png', dpi=1200)\n                                      \n    ldl = LayerDeepLift(model, model.encoder2)\n    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Encoder2\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer DeepLift_Encoder2.png', format='png', dpi=1200)\n                                      \n    ldl = LayerDeepLift(model, model.encoder3)\n    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Encoder3\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer DeepLift_Encoder3.png', format='png', dpi=1200)\n                                      \n    ldl = LayerDeepLift(model, model.encoder4)\n    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Encoder4\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer DeepLift_Encoder4.png', format='png', dpi=1200)\n                                      \n    ldl = LayerDeepLift(model, model.bottleneck)\n    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Bottleneck\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer DeepLift_Bottleneck.png', format='png', dpi=1200)\n                                      \n    ldl = LayerDeepLift(model, model.decoder4)\n    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Decoder4\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer DeepLift_Decoder4.png', format='png', dpi=1200)\n                                      \n    ldl = LayerDeepLift(model, model.decoder3)\n    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Decoder3\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer DeepLift_Decoder3.png', format='png', dpi=1200)\n                                      \n    ldl = LayerDeepLift(model, model.decoder2)\n    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu,signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Decoder2\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer DeepLift_Decoder2.png', format='png', dpi=1200)\n                                      \n    ldl = LayerDeepLift(model, model.decoder1)\n    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Decoder1\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer DeepLift_Decoder1.png', format='png', dpi=1200)\n                                      \n    ldl = LayerDeepLift(model, model.conv)\n    ldl_out = get_attr(ldl, img_batch, get_border(binary_mask))\n    figure, _ = viz.visualize_image_attr_multiple(agg_attr(ldl_out, image.shape[1]), original_image=img_cpu, signs=[\"all\"], \n                                                  methods=[\"blended_heat_map\"], show_colorbar=True, titles=[\"DeepLift - Last Layer\"], fig_size=figure_size)\n    figure.savefig(image_name + '_Layer DeepLift_LastLayer.png', format='png', dpi=1200)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T21:41:48.523576Z","iopub.execute_input":"2022-02-02T21:41:48.523826Z","iopub.status.idle":"2022-02-02T21:41:48.631643Z","shell.execute_reply.started":"2022-02-02T21:41:48.523798Z","shell.execute_reply":"2022-02-02T21:41:48.630984Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate Interpretions","metadata":{}},{"cell_type":"code","source":"def perturb_fn(inputs):\n    noise = torch.tensor(np.random.normal(0, 0.001, inputs.shape)).float().to(device)\n    return noise, inputs - noise","metadata":{"execution":{"iopub.status.busy":"2022-02-02T18:34:27.204437Z","iopub.execute_input":"2022-02-02T18:34:27.204723Z","iopub.status.idle":"2022-02-02T18:34:27.264593Z","shell.execute_reply.started":"2022-02-02T18:34:27.204691Z","shell.execute_reply":"2022-02-02T18:34:27.263423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def infidelity_score_model_interpretations(model, image):\n    \n    methods = [Saliency(model), GuidedBackprop(model), GuidedGradCam(model, model.conv), DeepLift(model)]\n    infidelity_scores = [0, 0, 0, 0]\n    \n    img_cpu = image.cpu().permute(1, 2, 0).detach().numpy()\n    img_batch = image.unsqueeze(0)\n    pred_mask = model(img_batch.to(device))[0]\n    binary_mask = torch.where(pred_mask >= 0.3, 1, 0)\n    border = get_border(binary_mask)\n    \n    for i in range(len(methods)):\n        infid = 0\n        for j in range(len(border)):\n            if isinstance(methods[i], DeepLift):\n                attribution = methods[i].attribute(img_batch.to(device), target=border[j], return_convergence_delta=False)\n            else:\n                attribution = methods[i].attribute(img_batch.to(device), target=border[j])\n            infid += infidelity(model, perturb_fn, img_batch.to(device), attribution, n_perturb_samples=1)\n        infid /= len(border)\n        infidelity_scores[i] += infid\n    \n    return infidelity_scores","metadata":{"execution":{"iopub.status.busy":"2022-02-02T16:40:17.761287Z","iopub.execute_input":"2022-02-02T16:40:17.761896Z","iopub.status.idle":"2022-02-02T16:40:17.826401Z","shell.execute_reply.started":"2022-02-02T16:40:17.76186Z","shell.execute_reply":"2022-02-02T16:40:17.825412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def infidelity_score_layer_interpretations(model, test_dataset):\n    \n    methods = [[LayerGradCam(model, model.encoder1), LayerGradCam(model, model.encoder2), LayerGradCam(model, model.encoder3), \n                LayerGradCam(model, model.encoder4), LayerGradCam(model, model.bottleneck), LayerGradCam(model, model.decoder4), \n                LayerGradCam(model, model.decoder3), LayerGradCam(model, model.decoder2), LayerGradCam(model, model.decoder1), \n                LayerGradCam(model, model.conv)], \n               [LayerDeepLift(model, model.encoder1), LayerDeepLift(model, model.encoder2), LayerDeepLift(model, model.encoder3), \n                LayerDeepLift(model, model.encoder4), LayerDeepLift(model, model.bottleneck), LayerDeepLift(model, model.decoder4), \n                LayerDeepLift(model, model.decoder3), LayerDeepLift(model, model.decoder2), LayerDeepLift(model, model.decoder1), \n                LayerDeepLift(model, model.conv)]]\n    infidelity_scores = [0, 0]\n    \n    img_cpu = image.cpu().permute(1, 2, 0).detach().numpy()\n    img_batch = image.unsqueeze(0)\n    pred_mask = model(img_batch.to(device))[0]\n    binary_mask = torch.where(pred_mask >= 0.3, 1, 0)\n    border = get_border(binary_mask)\n        \n    for i in range(len(methods)):\n        infid = 0\n        for k in range(len(methods[i])):\n            layer_infid = 0\n            for j in range(len(border)):\n                attribution = methods[i][k].attribute(img_batch.to(device), target=border[j])\n                if attribution.shape[2] < image.shape[1]:\n                    attribution = LayerAttribution.interpolate(attribution, (image.shape[1], image.shape[2]))\n                if isinstance(methods[i][k], LayerDeepLift):\n                    attribution = torch.mean(attribution, 1, keepdims=True)\n                layer_infid += infidelity(model, perturb_fn, img_batch.to(device), attribution.repeat(1, 3, 1, 1), n_perturb_samples=1)\n            layer_infid /= len(border)\n            infid += layer_infid\n        infid /= len(methods[i])\n        infidelity_scores[i] += infid\n    \n    return infidelity_scores","metadata":{"execution":{"iopub.status.busy":"2022-02-02T18:34:34.986144Z","iopub.execute_input":"2022-02-02T18:34:34.987093Z","iopub.status.idle":"2022-02-02T18:34:35.059782Z","shell.execute_reply.started":"2022-02-02T18:34:34.987052Z","shell.execute_reply":"2022-02-02T18:34:35.058677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_infid = []\nfor idx in range(len(dataset['test'])):\n    img, _ = dataset['test'][idx]\n    model_infid.append(infidelity_score_model_interpretations(best_model, img))","metadata":{"execution":{"iopub.status.busy":"2022-02-02T16:41:15.222382Z","iopub.execute_input":"2022-02-02T16:41:15.223176Z","iopub.status.idle":"2022-02-02T17:07:46.126458Z","shell.execute_reply.started":"2022-02-02T16:41:15.223126Z","shell.execute_reply":"2022-02-02T17:07:46.125515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layer_infid = []\nfor idx in range(len(dataset['test'])):\n    img, _ = dataset['test'][idx]\n    layer_infid.append(infidelity_score_layer_interpretations(best_model, img))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-02T18:34:39.831276Z","iopub.execute_input":"2022-02-02T18:34:39.83195Z","iopub.status.idle":"2022-02-02T20:30:00.83779Z","shell.execute_reply.started":"2022-02-02T18:34:39.83189Z","shell.execute_reply":"2022-02-02T20:30:00.836726Z"},"trusted":true},"execution_count":null,"outputs":[]}]}