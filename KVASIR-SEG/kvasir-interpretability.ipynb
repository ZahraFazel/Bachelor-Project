{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install imutils\n!pip install segmentation_models_pytorch\n!pip install captum\n!pip install albumentations\n!pip install gdown \nimport gdown \nurl = 'https://drive.google.com/uc?id=1LfVKeX5eY2pPrbQxxIjcLljls5uoexwB' \noutput = 'data.zip'\ngdown.download(url, output)\nurl = 'https://drive.google.com/uc?id=1Y93BKeKrvlwrieQc9aPkrvzwaUjWwE2Z' \noutput = 'best_model.pth'\ngdown.download(url, output)\n!unzip data.zip","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-29T20:27:48.363817Z","iopub.execute_input":"2022-01-29T20:27:48.364060Z","iopub.status.idle":"2022-01-29T20:29:02.835822Z","shell.execute_reply.started":"2022-01-29T20:27:48.363987Z","shell.execute_reply":"2022-01-29T20:29:02.835015Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n\nimport os\nfrom collections import defaultdict, OrderedDict\nimport shutil\nimport time\nimport copy\nimport math\nimport random\nfrom imutils import paths\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy import unravel_index\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nfrom torchvision import transforms\nfrom torchvision import datasets\n\nfrom PIL import *\nimport albumentations as A\nimport skimage\n\nimport segmentation_models_pytorch as smp\n\nprint(torch.cuda.is_available())","metadata":{"id":"SUEOkB6EoUAA","outputId":"d0151148-10bd-4b6b-98f3-27d3aec82203","execution":{"iopub.status.busy":"2022-01-29T20:29:02.837689Z","iopub.execute_input":"2022-01-29T20:29:02.837900Z","iopub.status.idle":"2022-01-29T20:29:08.460601Z","shell.execute_reply.started":"2022-01-29T20:29:02.837873Z","shell.execute_reply":"2022-01-29T20:29:08.459795Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"os.mkdir('./HyperKvasir/train')\nos.mkdir('./HyperKvasir/train/images')\nos.mkdir('./HyperKvasir/train/masks')\nos.mkdir('./HyperKvasir/val')\nos.mkdir('./HyperKvasir/val/images')\nos.mkdir('./HyperKvasir/val/masks')\nos.mkdir('./HyperKvasir/test')\nos.mkdir('./HyperKvasir/test/images')\nos.mkdir('./HyperKvasir/test/masks')\n# a = np.random.permutation(range(1, 1001))\na = np.array(range(1, 1001))\ntrain, val, test = a[:800], a[800:900], a[900:]\nfor i in train:\n    file_name = '{:04d}.jpg'.format(i)\n    shutil.copy('./HyperKvasir/images/' + file_name, './HyperKvasir/train/images/' + file_name)\n    shutil.copy('./HyperKvasir/masks/' + file_name, './HyperKvasir/train/masks/' + file_name)\nfor i in val:\n    file_name = '{:04d}.jpg'.format(i)\n    shutil.copy('./HyperKvasir/images/' + file_name, './HyperKvasir/val/images/' + file_name)\n    shutil.copy('./HyperKvasir/masks/' + file_name, './HyperKvasir/val/masks/' + file_name)\nfor i in test:\n    file_name = '{:04d}.jpg'.format(i)\n    shutil.copy('./HyperKvasir/images/' + file_name, './HyperKvasir/test/images/' + file_name)\n    shutil.copy('./HyperKvasir/masks/' + file_name, './HyperKvasir/test/masks/' + file_name)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T20:29:08.462377Z","iopub.execute_input":"2022-01-29T20:29:08.462893Z","iopub.status.idle":"2022-01-29T20:29:08.723323Z","shell.execute_reply.started":"2022-01-29T20:29:08.462843Z","shell.execute_reply":"2022-01-29T20:29:08.722524Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def visualize(**images):\n    n_images = len(images)\n    f, axarr = plt.subplots(1, n_images, figsize=(4 * n_images,4))\n    for idx, (name, image) in enumerate(images.items()):\n        if image.shape[0] == 3 or image.shape[0] == 2:\n            axarr[idx].imshow(np.squeeze(image.permute(1, 2, 0)))\n        else: \n            axarr[idx].imshow(np.squeeze(image))\n        axarr[idx].set_title(name.replace('_',' ').title(), fontsize=20)\n    plt.show()\n    \nclass EndoscopyDataset(Dataset):\n    def __init__(self, images, masks, augmentations=None):   \n        self.input_images = images\n        self.target_masks = masks\n        self.augmentations = augmentations\n\n    def __len__(self):\n        return len(self.input_images)\n    \n    def __getitem__(self, idx): \n        img = Image.open(os.path.join(self.input_images[idx])).convert('RGB')\n        mask = Image.open(os.path.join(self.target_masks[idx])).convert('RGB')\n        img = transforms.Compose([transforms.Resize((400, 400), interpolation=transforms.InterpolationMode.NEAREST), transforms.ToTensor()])(img)\n        mask = transforms.Compose([transforms.Resize((400, 400), interpolation=transforms.InterpolationMode.NEAREST), transforms.Grayscale(), transforms.ToTensor()])(mask)\n        img = img.permute((1, 2, 0))\n        mask = mask.permute((1, 2, 0))\n        img = img.cpu().detach().numpy()\n        mask = mask.cpu().detach().numpy()\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=img, mask=mask)\n            img = augmented['image']\n            mask = augmented['mask']\n        \n        img = torch.tensor(img, dtype=torch.float)\n        img = img.permute((2, 0, 1))\n        mask = torch.tensor(mask, dtype=torch.float)\n        mask = mask.permute((2, 0, 1))\n        \n        return [img, mask]\n    \ntrain_batch_size = 8\nval_batch_size = 4\ntest_batch_size = 4\nnum_workers = 2\n\n# main_dir = '/media/external_3TB/3TB/rasekh/fazel/KVASIR/'\n# main_dir = '/content/drive/My Drive/KVASIR/'\n# main_dir = 'KVASIR/'\nmain_dir = './HyperKvasir/'\n\ntrain_images = sorted(list(paths.list_files(main_dir + 'train/images/', contains=\"jpg\")))\nval_images = sorted(list(paths.list_files(main_dir + 'val/images/', contains=\"jpg\")))\ntest_images = sorted(list(paths.list_files(main_dir + 'test/images/', contains=\"jpg\")))\n\ntrain_masks = sorted(list(paths.list_files(main_dir + 'train/masks/', contains=\"jpg\")))\nval_masks = sorted(list(paths.list_files(main_dir + 'val/masks/', contains=\"jpg\")))\ntest_masks = sorted(list(paths.list_files(main_dir + 'test/masks/', contains=\"jpg\")))\n\naugmentations = A.Compose({\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(limit=(-90, 90)),\n        A.VerticalFlip(p=0.5),\n        A.Transpose(p=0.5),\n        A.GaussianBlur(p=0.5),\n        A.augmentations.geometric.transforms.Affine(scale=(0.9, 1.1), translate_percent=0.1)\n})\n\ndataset = {\n    'train': EndoscopyDataset(train_images, train_masks, augmentations), \n    'val': EndoscopyDataset(val_images, val_masks, None), \n    'test': EndoscopyDataset(test_images, test_masks, None)\n}\n\ndataloader = {\n    'train': DataLoader(dataset['train'], batch_size=train_batch_size, shuffle=True, num_workers=num_workers),\n    'val': DataLoader(dataset['val'], batch_size=val_batch_size, shuffle=True, num_workers=num_workers),\n    'test': DataLoader(dataset['test'], batch_size=test_batch_size, shuffle=False, num_workers=num_workers)\n}\n\nimage, mask = dataset['train'][random.randint(0, len(dataset['train'])-1)]\nprint(image.shape, image.min(), image.max())\nprint(mask.shape, mask.min(), mask.max())\nvisualize(\n    original_image = image,\n    grund_truth_mask = mask,\n    polyp = skimage.segmentation.mark_boundaries(image.permute(1, 2, 0).detach().cpu().numpy(), mask.detach().cpu().numpy()[0].astype(np.int64), color=(0, 0, 1), mode='outer')\n)","metadata":{"id":"dOKGH2G-oUAL","execution":{"iopub.status.busy":"2022-01-29T20:29:08.725962Z","iopub.execute_input":"2022-01-29T20:29:08.726224Z","iopub.status.idle":"2022-01-29T20:29:09.335085Z","shell.execute_reply.started":"2022-01-29T20:29:08.726190Z","shell.execute_reply":"2022-01-29T20:29:09.334435Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbest_model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\nbest_model = torch.load('./best_model.pth')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:24:36.749766Z","iopub.execute_input":"2022-01-29T21:24:36.750022Z","iopub.status.idle":"2022-01-29T21:24:36.908945Z","shell.execute_reply.started":"2022-01-29T21:24:36.749990Z","shell.execute_reply":"2022-01-29T21:24:36.908173Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nbest_model.eval()\n\nIOUs = []\nF1s = []\npredictions = []\n\nwith torch.no_grad():\n    for i, (inputs, labels) in enumerate(dataloader['test']):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        pred_mask = best_model(inputs)\n\n        for i in range(len(inputs)):\n            test_image = inputs[i]\n            test_mask = labels[i]\n            predMask = pred_mask[i]\n            \n            iou = smp.utils.functional.iou(predMask, test_mask, threshold=0.3)\n            IOUs.append(iou.cpu().detach())\n\n            f1 = smp.utils.functional.f_score(predMask, test_mask, threshold=0.3)\n            F1s.append(f1.cpu().detach())\n            \n            predMask = torch.where(predMask >= 0.3, 1, 0)\n            predictions.append(predMask)\n\n#             visualize(\n#                 original_image = test_image.cpu(),\n#                 ground_truth_mask = test_mask.cpu(),\n#                 predicted_mask = predMask.cpu(),\n#                 polyp = skimage.segmentation.mark_boundaries(test_image.permute(1, 2, 0).detach().cpu().numpy(), predMask.detach().cpu().numpy()[0].astype(np.int64), color=(0, 0, 1), mode='outer')\n#             )","metadata":{"id":"pj0hVHuioUAW","outputId":"aae25f2b-f76f-40e0-9825-4f61117fa483","scrolled":true,"execution":{"iopub.status.busy":"2022-01-29T21:46:30.450401Z","iopub.execute_input":"2022-01-29T21:46:30.450716Z","iopub.status.idle":"2022-01-29T21:47:28.820024Z","shell.execute_reply.started":"2022-01-29T21:46:30.450683Z","shell.execute_reply":"2022-01-29T21:47:28.818999Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"print('Test IOU: ' + str(np.mean(IOUs)))\nprint('Test F1: ' + str(np.mean(F1s)))","metadata":{"id":"8IfIxNS8oUAX","outputId":"18e27cec-5791-4717-d384-c8f852533409","execution":{"iopub.status.busy":"2022-01-29T21:47:28.824093Z","iopub.execute_input":"2022-01-29T21:47:28.824343Z","iopub.status.idle":"2022-01-29T21:47:28.881664Z","shell.execute_reply.started":"2022-01-29T21:47:28.824316Z","shell.execute_reply":"2022-01-29T21:47:28.880732Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"def segmentation_wrapper(inp):\n    # inp : tensor (3, 400, 400)\n    pred_mask = best_model(img.reshape((1, 3, 400, 400)).to(device))[0]\n    pred_mask = torch.where(pred_mask >= 0.3, 1, 0)\n    temp = np.stack(((1 - pred_mask)[0].detach().cpu().numpy(), pred_mask[0].detach().cpu().numpy())).reshape((1, 2, 400, 400))\n    model_out = torch.tensor(temp).type(torch.FloatTensor).to(device)\n    model_out.requires_grad_(True)\n    selected_inds = torch.zeros_like(model_out[0:1]).scatter_(1, pred_mask.long().reshape((1, 1, 400, 400)), 1)\n    return (model_out * selected_inds).sum(dim=(2,3))\n#     border = skimage.segmentation.find_boundaries(pred_mask).astype(np.uint8)\n#     indices = np.where(border == 1)\n#     concated = np.concatenate((indices[0][...,np.newaxis],indices[1][...,np.newaxis]),axis=1)\n#     borders = pred_mask[concated[:,0], concated[:,1]]\n#     return torch.Tensor([[np.where(borders == 0)[0].shape[0], np.where(borders == 1)[0].shape[0]]])","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:53:26.206717Z","iopub.execute_input":"2022-01-29T21:53:26.207003Z","iopub.status.idle":"2022-01-29T21:53:26.257984Z","shell.execute_reply.started":"2022-01-29T21:53:26.206975Z","shell.execute_reply":"2022-01-29T21:53:26.257290Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"index = np.argmax(IOUs)\nimg, mask = dataset['test'][index]","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:48:14.705211Z","iopub.execute_input":"2022-01-29T21:48:14.705909Z","iopub.status.idle":"2022-01-29T21:48:14.766452Z","shell.execute_reply.started":"2022-01-29T21:48:14.705873Z","shell.execute_reply":"2022-01-29T21:48:14.765759Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"from captum.attr import visualization as viz\nfrom captum.attr import LayerGradCam, FeatureAblation, LayerActivation, LayerAttribution","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:12:12.512471Z","iopub.execute_input":"2022-01-29T21:12:12.512743Z","iopub.status.idle":"2022-01-29T21:12:12.657321Z","shell.execute_reply.started":"2022-01-29T21:12:12.512715Z","shell.execute_reply":"2022-01-29T21:12:12.656650Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"lgc = LayerGradCam(segmentation_wrapper, best_model.encoder1)\ngc_attr = lgc.attribute(img, target=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:53:54.526236Z","iopub.execute_input":"2022-01-29T21:53:54.526531Z","iopub.status.idle":"2022-01-29T21:53:54.609495Z","shell.execute_reply.started":"2022-01-29T21:53:54.526502Z","shell.execute_reply":"2022-01-29T21:53:54.608530Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"la = LayerActivation(segmentation_wrapper, best_model.encoder1)\nactivation = la.attribute(img)\nprint(\"Input Shape:\", img.shape)\nprint(\"Layer Activation Shape:\", activation.shape)\nprint(\"Layer GradCAM Shape:\", gc_attr.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"viz.visualize_image_attr(gc_attr[0].cpu().permute(1,2,0).detach().numpy(),sign=\"all\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"upsampled_gc_attr = LayerAttribution.interpolate(gc_attr,normalized_inp.shape[2:])\nprint(\"Upsampled Shape:\",upsampled_gc_attr.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"viz.visualize_image_attr_multiple(upsampled_gc_attr[0].cpu().permute(1,2,0).detach().numpy(),original_image=preproc_img.permute(1,2,0).numpy(),signs=[\"all\", \"positive\", \"negative\"],methods=[\"original_image\", \"blended_heat_map\",\"blended_heat_map\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_without_train = (1 - (out_max == 19).float())[0].cpu() * preproc_img\nplt.imshow(img_without_train.permute(1,2,0))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fa = FeatureAblation(agg_segmentation_wrapper)\nfa_attr = fa.attribute(normalized_inp, feature_mask=out_max, perturbations_per_eval=2, target=6)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"viz.visualize_image_attr(fa_attr[0].cpu().detach().permute(1,2,0).numpy(),sign=\"all\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fa_attr_without_max = (1 - (out_max == 6).float())[0] * fa_attr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"viz.visualize_image_attr(fa_attr_without_max[0].cpu().detach().permute(1,2,0).numpy(),sign=\"all\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}