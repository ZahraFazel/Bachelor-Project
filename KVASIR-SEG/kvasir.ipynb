{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries and Download Data","metadata":{"execution":{"iopub.status.busy":"2022-01-29T17:21:24.17809Z","iopub.execute_input":"2022-01-29T17:21:24.178652Z","iopub.status.idle":"2022-01-29T17:22:27.40129Z","shell.execute_reply.started":"2022-01-29T17:21:24.17855Z","shell.execute_reply":"2022-01-29T17:22:27.400261Z"}}},{"cell_type":"code","source":"!pip install imutils\n!pip install segmentation_models_pytorch\n!pip install captum\n!pip install albumentations\n!pip install gdown \nimport gdown \nurl = 'https://drive.google.com/uc?id=1LfVKeX5eY2pPrbQxxIjcLljls5uoexwB' \noutput = 'data.zip'\ngdown.download(url, output)\n!unzip data.zip","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n\nimport os\nfrom collections import defaultdict, OrderedDict\nimport shutil\nimport time\nimport copy\nimport math\nimport random\nfrom imutils import paths\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy import unravel_index\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nfrom torchvision import transforms\nfrom torchvision import datasets\n\nfrom PIL import *\nimport albumentations as A\nimport skimage\n\nimport segmentation_models_pytorch as smp\n\nprint(torch.cuda.is_available())","metadata":{"id":"SUEOkB6EoUAA","outputId":"d0151148-10bd-4b6b-98f3-27d3aec82203","execution":{"iopub.status.busy":"2022-01-29T17:22:27.403345Z","iopub.execute_input":"2022-01-29T17:22:27.403578Z","iopub.status.idle":"2022-01-29T17:22:36.721502Z","shell.execute_reply.started":"2022-01-29T17:22:27.403547Z","shell.execute_reply":"2022-01-29T17:22:36.720015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"def visualize(**images):\n    n_images = len(images)\n    f, axarr = plt.subplots(1, n_images, figsize=(4 * n_images,4))\n    for idx, (name, image) in enumerate(images.items()):\n        if image.shape[0] == 3 or image.shape[0] == 2:\n            axarr[idx].imshow(np.squeeze(image.permute(1, 2, 0)))\n        else: \n            axarr[idx].imshow(np.squeeze(image))\n        axarr[idx].set_title(name.replace('_',' ').title(), fontsize=20)\n    plt.show()\n    \nclass EndoscopyDataset(Dataset):\n    def __init__(self, images, masks, augmentations=None):   \n        self.input_images = images\n        self.target_masks = masks\n        self.augmentations = augmentations\n\n    def __len__(self):\n        return len(self.input_images)\n    \n    def __getitem__(self, idx): \n        img = Image.open(os.path.join(self.input_images[idx])).convert('RGB')\n        mask = Image.open(os.path.join(self.target_masks[idx])).convert('RGB')\n        img = transforms.Compose([transforms.Resize((400, 400), interpolation=transforms.InterpolationMode.NEAREST), transforms.ToTensor()])(img)\n        mask = transforms.Compose([transforms.Resize((400, 400), interpolation=transforms.InterpolationMode.NEAREST), transforms.Grayscale(), transforms.ToTensor()])(mask)\n        img = img.permute((1, 2, 0))\n        mask = mask.permute((1, 2, 0))\n        img = img.cpu().detach().numpy()\n        mask = mask.cpu().detach().numpy()\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=img, mask=mask)\n            img = augmented['image']\n            mask = augmented['mask']\n        \n        img = torch.tensor(img, dtype=torch.float)\n        img = img.permute((2, 0, 1))\n        mask = torch.tensor(mask, dtype=torch.float)\n        mask = mask.permute((2, 0, 1))\n        \n        return [img, mask]\n    \ntrain_batch_size = 8\nval_batch_size = 4\ntest_batch_size = 4\nnum_workers = 2\n\nmain_dir = './'\n\ntrain_images = sorted(list(paths.list_files(main_dir + 'train/images/', contains=\"jpg\")))\nval_images = sorted(list(paths.list_files(main_dir + 'val/images/', contains=\"jpg\")))\ntest_images = sorted(list(paths.list_files(main_dir + 'test/images/', contains=\"jpg\")))\n\ntrain_masks = sorted(list(paths.list_files(main_dir + 'train/masks/', contains=\"jpg\")))\nval_masks = sorted(list(paths.list_files(main_dir + 'val/masks/', contains=\"jpg\")))\ntest_masks = sorted(list(paths.list_files(main_dir + 'test/masks/', contains=\"jpg\")))\n\naugmentations = A.Compose({\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(limit=(-90, 90)),\n        A.VerticalFlip(p=0.5),\n        A.Transpose(p=0.5),\n        A.GaussianBlur(p=0.5),\n        A.augmentations.geometric.transforms.Affine(scale=(0.9, 1.1), translate_percent=0.1)\n})\n\ndataset = {\n    'train': EndoscopyDataset(train_images, train_masks, augmentations), \n    'val': EndoscopyDataset(val_images, val_masks, None), \n    'test': EndoscopyDataset(test_images, test_masks, None)\n}\n\ndataloader = {\n    'train': DataLoader(dataset['train'], batch_size=train_batch_size, shuffle=True, num_workers=num_workers),\n    'val': DataLoader(dataset['val'], batch_size=val_batch_size, shuffle=True, num_workers=num_workers),\n    'test': DataLoader(dataset['test'], batch_size=test_batch_size, shuffle=False, num_workers=num_workers)\n}\n\nimage, mask = dataset['train'][random.randint(0, len(dataset['train'])-1)]\nprint(image.shape, image.min(), image.max())\nprint(mask.shape, mask.min(), mask.max())\nvisualize(\n    original_image = image,\n    grund_truth_mask = mask,\n    polyp = skimage.segmentation.mark_boundaries(image.permute(1, 2, 0).detach().cpu().numpy(), mask.detach().cpu().numpy()[0].astype(np.int64), color=(0, 0, 1), mode='outer')\n)","metadata":{"id":"dOKGH2G-oUAL","execution":{"iopub.status.busy":"2022-01-29T17:22:37.009593Z","iopub.execute_input":"2022-01-29T17:22:37.009861Z","iopub.status.idle":"2022-01-29T17:22:37.525259Z","shell.execute_reply.started":"2022-01-29T17:22:37.009827Z","shell.execute_reply":"2022-01-29T17:22:37.523242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"class UNet(nn.Module):\n\n    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n        super(UNet, self).__init__()\n\n        features = init_features\n        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n\n        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n        self.conv = nn.Conv2d(in_channels=features, out_channels=out_channels, kernel_size=1)\n\n    def forward(self, x):\n        enc1 = self.encoder1(x)\n        enc2 = self.encoder2(self.pool1(enc1))\n        enc3 = self.encoder3(self.pool2(enc2))\n        enc4 = self.encoder4(self.pool3(enc3))\n\n        bottleneck = self.bottleneck(self.pool4(enc4))\n\n        dec4 = self.upconv4(bottleneck)\n        dec4 = torch.cat((dec4, enc4), dim=1)\n        dec4 = self.decoder4(dec4)\n        dec3 = self.upconv3(dec4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n        dec2 = self.upconv2(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n        dec1 = self.upconv1(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n        return torch.sigmoid(self.conv(dec1))\n\n    @staticmethod\n    def _block(in_channels, features, name):\n        return nn.Sequential(\n            OrderedDict(\n                [\n                    (\n                        name + \"conv1\",\n                        nn.Conv2d(\n                            in_channels=in_channels,\n                            out_channels=features,\n                            kernel_size=3,\n                            padding=1,\n                            bias=False,\n                        ),\n                    ),\n                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n                    (name + \"relu1\", nn.ReLU(inplace=True)),\n                    (\n                        name + \"conv2\",\n                        nn.Conv2d(\n                            in_channels=features,\n                            out_channels=features,\n                            kernel_size=3,\n                            padding=1,\n                            bias=False,\n                        ),\n                    ),\n                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n                    (name + \"relu2\", nn.ReLU(inplace=True)),\n                ]\n            )\n        )\n\nclass Loss(smp.utils.base.Loss):\n    def __init__(self, eps=1.0, activation=None, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.activation = smp.base.modules.Activation(activation)\n        self._name = 'loss'\n\n    def forward(self, y_pr, y_gt):\n        y_pr = self.activation(y_pr)\n        return (1 - smp.utils.functional.iou(y_pr, y_gt, eps=self.eps, threshold=0.3)) + (1 - smp.utils.functional.f_score(y_pr, y_gt, beta=1, eps=self.eps, threshold=0.3)) + nn.functional.binary_cross_entropy(y_pr, y_gt)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T17:22:37.526259Z","iopub.execute_input":"2022-01-29T17:22:37.526498Z","iopub.status.idle":"2022-01-29T17:22:37.61095Z","shell.execute_reply.started":"2022-01-29T17:22:37.526459Z","shell.execute_reply":"2022-01-29T17:22:37.610254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training = True\nepochs = 500\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)\n\nloss = Loss()\n\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.3),\n    smp.utils.metrics.Fscore(threshold=0.3),\n]\n\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.0005),\n])\n\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n    optimizer, T_0=1, eta_min=0.00001,\n)\n\ntrain_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=device,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=device,\n    verbose=True,\n)\n\nif training:\n\n    best_model = {'loss': 0.0, 'iou_score': 0.0, 'fscore': 0.0}\n    train_logs_list, valid_logs_list = [], []\n\n    for i in range(0, epochs):\n        print('\\nEpoch: {}'.format(i))\n        train_logs = train_epoch.run(dataloader['train'])\n        valid_logs = valid_epoch.run(dataloader['val'])\n        train_logs_list.append(train_logs)\n        valid_logs_list.append(valid_logs)\n\n        if best_model['iou_score'] < valid_logs['iou_score']:\n            torch.save(model, main_dir + 'best_model.pth')\n            best_model['loss'] = valid_logs['loss']\n            best_model['iou_score'] = valid_logs['iou_score']\n            best_model['fscore'] = valid_logs['fscore']\n            print('Model saved!')","metadata":{"id":"fIlVc7OapTuX","scrolled":true,"execution":{"iopub.status.busy":"2022-01-29T17:22:37.612384Z","iopub.execute_input":"2022-01-29T17:22:37.612646Z","iopub.status.idle":"2022-01-29T21:30:52.383931Z","shell.execute_reply.started":"2022-01-29T17:22:37.61261Z","shell.execute_reply":"2022-01-29T21:30:52.382486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(best_model)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:33:37.151656Z","iopub.execute_input":"2022-01-29T21:33:37.152346Z","iopub.status.idle":"2022-01-29T21:33:37.23232Z","shell.execute_reply.started":"2022-01-29T21:33:37.1523Z","shell.execute_reply":"2022-01-29T21:33:37.231566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_logs_df = pd.DataFrame(train_logs_list)\nvalid_logs_df = pd.DataFrame(valid_logs_list)","metadata":{"id":"_B-5qHn-rO6_","outputId":"8c60ef1d-897e-4e5b-a94e-15ed79d5c870","execution":{"iopub.status.busy":"2022-01-29T21:33:40.560684Z","iopub.execute_input":"2022-01-29T21:33:40.561284Z","iopub.status.idle":"2022-01-29T21:33:40.634549Z","shell.execute_reply.started":"2022-01-29T21:33:40.561239Z","shell.execute_reply":"2022-01-29T21:33:40.6338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.plot(train_logs_df.index.tolist(), train_logs_df.loss.tolist(), lw=1, label = 'Train')\nplt.plot(valid_logs_df.index.tolist(), valid_logs_df.loss.tolist(), lw=1, label = 'Valid')\nplt.xlabel('Epochs', fontsize=20)\nplt.ylabel('Loss', fontsize=20)\nplt.title('Loss Plot', fontsize=20)\nplt.legend(loc='best', fontsize=16)\nplt.grid()\nplt.savefig('loss_plot.png')\nplt.show()","metadata":{"id":"zr2PPzT9oUAS","outputId":"aa60af71-7353-4469-d4de-88627abe7819","execution":{"iopub.status.busy":"2022-01-29T21:33:43.220511Z","iopub.execute_input":"2022-01-29T21:33:43.221145Z","iopub.status.idle":"2022-01-29T21:33:43.626557Z","shell.execute_reply.started":"2022-01-29T21:33:43.221104Z","shell.execute_reply":"2022-01-29T21:33:43.625863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=1, label = 'Train')\nplt.plot(valid_logs_df.index.tolist(), valid_logs_df.iou_score.tolist(), lw=1, label = 'Valid')\nplt.xlabel('Epochs', fontsize=20)\nplt.ylabel('IoU Score', fontsize=20)\nplt.title('IoU Score Plot', fontsize=20)\nplt.legend(loc='best', fontsize=16)\nplt.grid()\nplt.savefig('iou_score_plot.png')\nplt.show()","metadata":{"id":"L5arr_lnoUAU","outputId":"1b6cba91-25a1-4335-c45a-4aecbe88480f","execution":{"iopub.status.busy":"2022-01-29T21:33:45.540457Z","iopub.execute_input":"2022-01-29T21:33:45.541012Z","iopub.status.idle":"2022-01-29T21:33:45.933445Z","shell.execute_reply.started":"2022-01-29T21:33:45.540976Z","shell.execute_reply":"2022-01-29T21:33:45.932745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.plot(train_logs_df.index.tolist(), train_logs_df.fscore.tolist(), lw=1, label = 'Train')\nplt.plot(valid_logs_df.index.tolist(), valid_logs_df.fscore.tolist(), lw=1, label = 'Valid')\nplt.xlabel('Epochs', fontsize=20)\nplt.ylabel('F1 Score', fontsize=20)\nplt.title('F1 Score Plot', fontsize=20)\nplt.legend(loc='best', fontsize=16)\nplt.grid()\nplt.savefig('fscore_plot.png')\nplt.show()","metadata":{"id":"ZLrsVTU9oUAV","outputId":"1071d01d-b193-4909-d4d0-885c015bcc7d","execution":{"iopub.status.busy":"2022-01-29T21:33:52.672426Z","iopub.execute_input":"2022-01-29T21:33:52.673091Z","iopub.status.idle":"2022-01-29T21:33:53.069648Z","shell.execute_reply.started":"2022-01-29T21:33:52.673051Z","shell.execute_reply":"2022-01-29T21:33:53.068819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Model","metadata":{}},{"cell_type":"code","source":"best_model = torch.load('./HyperKvasir/best_model.pth')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:34:31.845828Z","iopub.execute_input":"2022-01-29T21:34:31.84608Z","iopub.status.idle":"2022-01-29T21:34:31.945612Z","shell.execute_reply.started":"2022-01-29T21:34:31.846053Z","shell.execute_reply":"2022-01-29T21:34:31.94491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nbest_model.eval()\n\nIOUs = []\nF1s = []\n\nwith torch.no_grad():\n    for i, (inputs, labels) in enumerate(dataloader['test']):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        pred_mask = best_model(inputs)\n\n        for i in range(len(inputs)):\n            test_image = inputs[i]\n            test_mask = labels[i]\n            predMask = pred_mask[i]\n            \n            iou = smp.utils.functional.iou(predMask, test_mask, threshold=0.3)\n            IOUs.append(iou.cpu().detach())\n\n            f1 = smp.utils.functional.f_score(predMask, test_mask, threshold=0.3)\n            F1s.append(f1.cpu().detach())\n            \n            predMask = torch.where(predMask >= 0.3, 1, 0)\n\n            visualize(\n                original_image = test_image.cpu(),\n                ground_truth_mask = test_mask.cpu(),\n                predicted_mask = predMask.cpu(),\n                polyp = skimage.segmentation.mark_boundaries(image.permute(1, 2, 0).detach().cpu().numpy(), mask.detach().cpu().numpy()[0].astype(np.int64), color=(0, 0, 1), mode='outer')\n            )","metadata":{"id":"pj0hVHuioUAW","outputId":"aae25f2b-f76f-40e0-9825-4f61117fa483","scrolled":true,"execution":{"iopub.status.busy":"2022-01-29T21:34:34.813182Z","iopub.execute_input":"2022-01-29T21:34:34.813931Z","iopub.status.idle":"2022-01-29T21:35:23.37742Z","shell.execute_reply.started":"2022-01-29T21:34:34.813895Z","shell.execute_reply":"2022-01-29T21:35:23.376575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Test IOU: ' + str(np.mean(IOUs)))\nprint('Test F1: ' + str(np.mean(F1s)))","metadata":{"id":"8IfIxNS8oUAX","outputId":"18e27cec-5791-4717-d384-c8f852533409","execution":{"iopub.status.busy":"2022-01-29T21:35:23.379136Z","iopub.execute_input":"2022-01-29T21:35:23.37943Z","iopub.status.idle":"2022-01-29T21:35:23.451756Z","shell.execute_reply.started":"2022-01-29T21:35:23.3794Z","shell.execute_reply":"2022-01-29T21:35:23.451035Z"},"trusted":true},"execution_count":null,"outputs":[]}]}